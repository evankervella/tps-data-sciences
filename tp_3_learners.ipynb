{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisation et Chargement des Iris\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added version check for recent scikit-learn 0.18 checks\n",
    "from distutils.version import LooseVersion as Version\n",
    "from sklearn import __version__ as sklearn_version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Iris dataset from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "('Class labels:', array([0, 1, 2]))\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "print iris.data.shape\n",
    "\n",
    "print('Class labels:', np.unique(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting data into 70% training and 30% test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Version(sklearn_version) < '0.18':\n",
    "    from sklearn.cross_validation import train_test_split\n",
    "else:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizing the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Le Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redefining the `plot_decision_region` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:73: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Perceptron(alpha=0.0001, class_weight=None, eta0=0.1, fit_intercept=True,\n",
       "      max_iter=40, n_iter=None, n_jobs=1, penalty=None, random_state=0,\n",
       "      shuffle=True, tol=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "ppn = Perceptron(n_iter=40, eta0=0.1, random_state=0)\n",
    "ppn.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45,)\n"
     ]
    }
   ],
   "source": [
    "print y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified samples: 8\n"
     ]
    }
   ],
   "source": [
    "y_pred = ppn.predict(X_test_std)\n",
    "print('Misclassified samples: %d' % (y_test != y_pred).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.82\n",
      "0.822222222222\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print ppn.score(X_test_std,y_test )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a perceptron model using the standardized training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_combined_std = np.vstack((X_train_std, X_test_std))\n",
    "y_combined = np.hstack((y_train, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K plus proches voisins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.933333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1, p=2, metric='minkowski')\n",
    "knn.fit(X_train_std, y_train)\n",
    "\n",
    "print knn.score(X_test_std, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arbres de décision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.98\n",
      "(105, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#tree = DecisionTreeClassifier(criterion='entropy', max_depth=4, random_state=0)\n",
    "\n",
    "tree = DecisionTreeClassifier(criterion='gini', splitter='best', )\n",
    "\n",
    "tree.fit(X_train_std, y_train)\n",
    "\n",
    "print('Accuracy: %.2f' % tree.score(X_test_std,y_test ))\n",
    "\n",
    "print X_train_std.shape\n",
    "\n",
    "X_combined = np.vstack((X_train_std, X_test_std))\n",
    "y_combined = np.hstack((y_train, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Régression logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(C=1000.0, random_state=0)\n",
    "lr.fit(X_train_std, y_train)\n",
    "\n",
    "\n",
    "print('Accuracy: %.2f' % lr.score(X_test_std,y_test ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Version(sklearn_version) < '0.17':\n",
    "    lr.predict_proba(X_test_std[0, :])\n",
    "else:\n",
    "    lr.predict_proba(X_test_std[0, :].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support vector machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Le cas non linéairement séparable et les slack variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(kernel='linear', C=1.0, random_state=0)\n",
    "svm.fit(X_train_std, y_train)\n",
    "\n",
    "\n",
    "print('Accuracy: %.2f' % svm.score(X_test_std,y_test ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Travail à réaliser\n",
    "\n",
    "Reproduisez pour les datasets suivants...\n",
    " - [Digits](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits) (en utilisant les données complètes)\n",
    " - [Iris](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html#sklearn.datasets.load_iris)\n",
    "\n",
    "... les expérimentations suivantes \n",
    "\n",
    "- Mise au point de plusieurs types de classifieurs (Perceptron, régression logistique, Arbre de décision, SVM). Pour chacun de ces types de classifieurs vous devrez :\n",
    " - Définir les hyper-paramètres à faire varier.\n",
    " - Evaluer et selectionner par Grid-Search l'ensemble des configurations possibles, en utilisant la Validation Croisée à 3 plis pour l'évaluation de la performance en généralisation. Vous pourrez vous inspirer d'un code tel que [celui-ci](http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html#sphx-glr-auto-examples-classification-plot-classifier-comparison-py) pour boucler sur les datasets et/ou les classifieurs.\n",
    "- Ecrire sous forme d'un tableau récapitulatif les performances respectives (les meilleures obtenues) par chacun des modèles sur chacun des jeux de données (sur le test set).\n",
    "- Donner des conclusions sur les résultats obtenus quand à la performance, la stabilité, la robustesse des familles de classifieurs utilisées, et les paramètres optimaux de chaque type de modèle.\n",
    "\n",
    "\n",
    "  \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search sur le Perceptron.\n",
    "On décide de tester l'optimalité sur les paramètres suivants : alpha\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:84: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier : 'Perceptron'\n",
      "Best score en train : 0.8952380952380953\n",
      "Best params : {'alpha': 0.001, 'max_iter': 50, 'eta0': 0.1, 'n_iter': 2}\n",
      "Misclassified samples: 7\n",
      "Accuracy: 0.84\n",
      "Score en test : 0.84444444444444444\n",
      "\n",
      "\n",
      "Classifier : 'KNeighborsClassifier'\n",
      "Best score en train : 0.9523809523809523\n",
      "Best params : {'n_neighbors': 5, 'weights': 'uniform', 'leaf_size': 20, 'p': 2}\n",
      "Misclassified samples: 1\n",
      "Accuracy: 0.98\n",
      "Score en test : 0.97777777777777775\n",
      "\n",
      "\n",
      "Classifier : 'LogisticRegression'\n",
      "Best score en train : 0.9523809523809523\n",
      "Best params : {'C': 500, 'random_state': 0}\n",
      "Misclassified samples: 1\n",
      "Accuracy: 0.98\n",
      "Score en test : 0.97777777777777775\n",
      "\n",
      "\n",
      "Classifier : 'DecisionTreeClassifier'\n",
      "Best score en train : 0.9619047619047619\n",
      "Best params : {'max_leaf_nodes': 5, 'max_depth': None, 'splitter': 'best'}\n",
      "Misclassified samples: 1\n",
      "Accuracy: 0.98\n",
      "Score en test : 0.97777777777777775\n",
      "\n",
      "\n",
      "Classifier : 'SVC'\n",
      "Best score en train : 0.9523809523809523\n",
      "Best params : {'kernel': 'rbf', 'C': 5, 'random_state': 0, 'gamma': 0.1}\n",
      "Misclassified samples: 1\n",
      "Accuracy: 0.98\n",
      "Score en test : 0.97777777777777775\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import grid_search\n",
    "\n",
    "# On crée la liste des classifiers, pour plus de clarté dans le code :\n",
    "classifiers = [\n",
    "    Perceptron(),\n",
    "    KNeighborsClassifier(),\n",
    "    LogisticRegression(),\n",
    "    DecisionTreeClassifier(),\n",
    "    SVC()\n",
    "    ]\n",
    "\n",
    "nom_classifiers = [\n",
    "    \"Perceptron\",\n",
    "    \"KNeighborsClassifier\",\n",
    "    \"LogisticRegression\",\n",
    "    \"DecisionTreeClassifier\",\n",
    "    \"SVC\"\n",
    "    ]\n",
    "\n",
    "#Idem avec les paramètres des classifiers ci-dessus. On peut ainsi facilement y avoir accès et les modifier :\n",
    "parameters = [\n",
    "    {'alpha':[0.001, 0.01, 0.1, 1], 'eta0':[0.1, 0.05, 0.01, 0.005], 'n_iter':[2, 5, 10, 10, 100],\n",
    "              'max_iter':[10, 20, 50, 100]},\n",
    "    {'n_neighbors':[1, 2, 5, 10], 'p':[1, 2, 5], 'weights':['uniform', 'distance'], 'leaf_size':[20, 30, 40, 50]},\n",
    "    {'C':[500, 1000, 2000], 'random_state':[0, 1, 2, 5]},\n",
    "    {'max_depth':[None, 1, 2, 3, 4], 'splitter':['best'], 'max_leaf_nodes':[None, 2, 5]},\n",
    "    {'kernel':['linear', 'rbf'], 'C':[1, 2, 5], 'gamma':[0.05, 0.1, 0.5, 1], 'random_state':[0, 1, 2]}\n",
    "    ]\n",
    "\n",
    "\n",
    "resultats_grid = []\n",
    "i = 0\n",
    "while i < len(classifiers):\n",
    "    clf = grid_search.GridSearchCV(classifiers[i], parameters[i], cv=None)   # cv = None => 3 folders par défaut\n",
    "    clf.fit(X_train_std, y_train)\n",
    "    y_pred = clf.predict(X_test_std)\n",
    "    #resultats_grid.append(\n",
    "     #   \"Classifier : %r => Meilleurs paramètres: %r, meilleur score: %r, misclassified samples: %r, accuracy: %.2f, score: %r\"\n",
    "      #                    % (classifiers[i], clf.best_params_, clf.best_score_, (y_test != y_pred).sum(),\n",
    "       #                       accuracy_score(y_test, y_pred), classifiers[i].score(X_test_std,y_test)))\n",
    "    \n",
    "    print ('Classifier : %r' % (nom_classifiers[i]))\n",
    "    print ('Best score en train : %r' % (clf.best_score_))\n",
    "    print ('Best params : %r' % (clf.best_params_))\n",
    "    print('Misclassified samples: %d' % (y_test != y_pred).sum())\n",
    "    print('Accuracy: %.2f' % (accuracy_score(y_test, y_pred)))\n",
    "    print ('Score en test : %r\\n\\n' % (clf.score(X_test_std,y_test)))\n",
    "    i += 1\n",
    "    \n",
    "#print resultats_grid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commentaires\n",
    "\n",
    "Les résultats après gridsearch sont meilleurs qu'avec les premiers paramètres fixés. Elle fut donc utile !\n",
    "Les résultats sont excellents (1 seule erreur en train en général, et un score en test proche de 98%).\n",
    "On remarque que le perceptron est le classifier le moins efficace, ce qui n'est pas surprenant puisqu'il est beaucoup moins complexe que les autres classifiers.\n",
    "\n",
    "Après avoir regardé les fiches sklearn des différents classifiers, on a fait varier les paramètres pertinents sur des valeurs du même ordre de grandeur que celle par défaut pour les grandeurs quantitatives. Pour les grandeurs qualitatives on a pu se permettre de les faire varier sur la plupart des valeurs possibles, l'ensemble étant fini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n",
      "('Class labels:', array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]))\n"
     ]
    }
   ],
   "source": [
    "digits = datasets.load_digits()\n",
    "\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "print digits.data.shape\n",
    "\n",
    "print('Class labels:', np.unique(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting data into 70% training and 30% test data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Version(sklearn_version) < '0.18':\n",
    "    from sklearn.cross_validation import train_test_split\n",
    "else:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizing the features :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier : 'Perceptron'\n",
      "Best score en train : 0.939538583929992\n",
      "Best params : {'alpha': 0.001, 'max_iter': 20, 'eta0': 0.1, 'n_iter': 2}\n",
      "Misclassified samples: 40\n",
      "Accuracy: 0.93\n",
      "Score en test : 0.92592592592592593\n",
      "\n",
      "\n",
      "Classifier : 'KNeighborsClassifier'\n",
      "Best score en train : 0.9721559268098647\n",
      "Best params : {'n_neighbors': 5, 'weights': 'distance', 'leaf_size': 20, 'p': 2}\n",
      "Misclassified samples: 17\n",
      "Accuracy: 0.97\n",
      "Score en test : 0.96851851851851856\n",
      "\n",
      "\n",
      "Classifier : 'LogisticRegression'\n",
      "Best score en train : 0.954653937947494\n",
      "Best params : {'C': 1000, 'random_state': 0}\n",
      "Misclassified samples: 35\n",
      "Accuracy: 0.94\n",
      "Score en test : 0.93518518518518523\n",
      "\n",
      "\n",
      "Classifier : 'DecisionTreeClassifier'\n",
      "Best score en train : 0.8273667462211615\n",
      "Best params : {'max_leaf_nodes': None, 'max_depth': None, 'splitter': 'best'}\n",
      "Misclassified samples: 94\n",
      "Accuracy: 0.83\n",
      "Score en test : 0.82592592592592595\n",
      "\n",
      "\n",
      "Classifier : 'SVC'\n",
      "Best score en train : 0.9729514717581543\n",
      "Best params : {'kernel': 'linear', 'C': 1, 'random_state': 0, 'gamma': 0.05}\n",
      "Misclassified samples: 16\n",
      "Accuracy: 0.97\n",
      "Score en test : 0.97037037037037033\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import grid_search\n",
    "\n",
    "# On crée la liste des classifiers, pour plus de clarté dans le code :\n",
    "classifiers = [\n",
    "    Perceptron(),\n",
    "    KNeighborsClassifier(),\n",
    "    LogisticRegression(),\n",
    "    DecisionTreeClassifier(),\n",
    "    SVC()\n",
    "    ]\n",
    "\n",
    "nom_classifiers = [\n",
    "    \"Perceptron\",\n",
    "    \"KNeighborsClassifier\",\n",
    "    \"LogisticRegression\",\n",
    "    \"DecisionTreeClassifier\",\n",
    "    \"SVC\"\n",
    "    ]\n",
    "\n",
    "#Idem avec les paramètres des classifiers ci-dessus. On peut ainsi facilement y avoir accès et les modifier :\n",
    "parameters = [\n",
    "    {'alpha':[0.001, 0.01, 0.1, 1], 'eta0':[0.1, 0.05, 0.01, 0.005], 'n_iter':[2, 5, 10, 10, 100],\n",
    "              'max_iter':[10, 20, 50, 100]},\n",
    "    {'n_neighbors':[1, 2, 5, 10], 'p':[1, 2, 5], 'weights':['uniform', 'distance'], 'leaf_size':[20, 30, 40, 50]},\n",
    "    {'C':[500, 1000, 2000], 'random_state':[0, 1, 2, 5]},\n",
    "    {'max_depth':[None, 1, 2, 3, 4], 'splitter':['best'], 'max_leaf_nodes':[None, 2, 5]},\n",
    "    {'kernel':['linear', 'rbf'], 'C':[1, 2, 5], 'gamma':[0.05, 0.1, 0.5, 1], 'random_state':[0, 1, 2]}\n",
    "    ]\n",
    "\n",
    "\n",
    "resultats_grid = []\n",
    "i = 0\n",
    "while i < len(classifiers):\n",
    "    clf = grid_search.GridSearchCV(classifiers[i], parameters[i], cv=None)\n",
    "    clf.fit(X_train_std, y_train)\n",
    "    y_pred = clf.predict(X_test_std)\n",
    "    #resultats_grid.append(\n",
    "     #   \"Classifier : %r => Meilleurs paramètres: %r, meilleur score: %r, misclassified samples: %r, accuracy: %.2f, score: %r\"\n",
    "      #                    % (classifiers[i], clf.best_params_, clf.best_score_, (y_test != y_pred).sum(),\n",
    "       #                       accuracy_score(y_test, y_pred), classifiers[i].score(X_test_std,y_test)))\n",
    "    \n",
    "    print ('Classifier : %r' % (nom_classifiers[i]))\n",
    "    print ('Best score en train : %r' % (clf.best_score_))\n",
    "    print ('Best params : %r' % (clf.best_params_))\n",
    "    print('Misclassified samples: %d' % (y_test != y_pred).sum())\n",
    "    print('Accuracy: %.2f' % (accuracy_score(y_test, y_pred)))\n",
    "    print ('Score en test : %r\\n\\n' % (clf.score(X_test_std,y_test)))\n",
    "    i += 1\n",
    "    \n",
    "#print resultats_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commentaire\n",
    "\n",
    "Les résultats après gridsearch sont meilleurs qu'avec les premiers paramètres fixés. Elle fut donc utile ! Les résultats sont excellents mais moins bons que sur Iris. C'est du au fait que le data frame iris est très simple, là où Digits est plus complexe.\n",
    "Sur Digits, DecisionTreeClassifier est de loin le moins bon classifier.\n",
    "Après avoir regardé les fiches sklearn des différents classifiers, on a fait varier les paramètres pertinents sur des valeurs du même ordre de grandeur que celle par défaut pour les grandeurs quantitatives. Pour les grandeurs qualitatives on a pu se permettre de les faire varier sur la plupart des valeurs possibles, l'ensemble étant fini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
